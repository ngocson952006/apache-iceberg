{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-30T01:46:58.003818Z",
     "start_time": "2025-08-30T01:46:57.886953Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T01:47:01.632216Z",
     "start_time": "2025-08-30T01:46:58.067264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "minio_endpoint = os.getenv(\"STORAGE_URI\", \"http://222.255.214.74:9000\")\n",
    "minio_access_key = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "minio_secret_key = os.getenv(\"MINIO_SECRET_KEY\", \"password\")\n",
    "MINIO_S3_FILE_PATH = \"s3a://csv-data-files/HR_Data_MNC_Data.csv\"\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"PostgreSQL ingestion from file\")\n",
    "         .config(\"spark.jars.packages\",\n",
    "                 \",\".join([\"org.postgresql:postgresql:42.7.4\",\n",
    "                           \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "                           \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "                           ]))\n",
    "         # Cap driver and executor heaps:\n",
    "         .config(\"spark.driver.memory\", \"2g\")\n",
    "         .config(\"spark.executor.memory\", \"2g\")\n",
    "         .getOrCreate())\n",
    "jdbc_url = os.getenv(\"JDBC_URL\", \"jdbc:postgresql://localhost:5435/mydb\")\n",
    "file_path = \"../../csv_files/HR_Data_MNC_Data.csv\"\n",
    "\n",
    "target_table = \"public.hr_data\"\n",
    "db_user = \"myuser\"\n",
    "db_password = \"mypassword\"\n",
    "\n",
    "use_minio = True\n",
    "\n",
    "if use_minio:\n",
    "    # Configure S3A to talk to MinIO\n",
    "    hconf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hconf.set(\"fs.s3a.endpoint\", minio_endpoint)  # MinIO endpoint\n",
    "    hconf.set(\"fs.s3a.access.key\", minio_access_key)\n",
    "    hconf.set(\"fs.s3a.secret.key\", minio_secret_key)\n",
    "    hconf.set(\"fs.s3a.path.style.access\", \"true\")  # required for MinIO\n",
    "    hconf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")  # set true if your MinIO uses HTTPS\n",
    "    hconf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n"
   ],
   "id": "16a8251f0b460d7f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 08:46:59 WARN Utils: Your hostname, MacBook-Air-cua-Ngoc-2.local resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "25/08/30 08:46:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/truongngocson/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/truongngocson/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87e7b32f-e3e8-408a-8f56-6537bba83321;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.4 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/truongngocson/Documents/Projects/apache-iceberg/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.checkerframework#checker-qual;3.42.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 185ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from local-m2-cache in [default]\n",
      "\torg.postgresql#postgresql;42.7.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87e7b32f-e3e8-408a-8f56-6537bba83321\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "25/08/30 08:46:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read data into dataframe",
   "id": "8fe2933a3b8b8443"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T01:47:31.328432Z",
     "start_time": "2025-08-30T01:47:01.682605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if use_minio:\n",
    "    print(\"Start reading file from MinIO\")\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")  # consider providing an explicit schema for production\n",
    "        .csv(MINIO_S3_FILE_PATH)\n",
    "    )\n",
    "else:\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.describe()\n",
    "df.head()"
   ],
   "id": "2abef0d0b78cf06a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading file from MinIO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 08:47:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Unnamed: 0=0, Employee_ID='EMP0000001', Full_Name='Joshua Nguyen', Department='IT', Job_Title='Software Engineer', Hire_Date=datetime.date(2011, 8, 10), Location='Isaacland, Denmark', Performance_Rating=5, Experience_Years=14, Status='Resigned', Work_Mode='On-site', Salary_INR=1585363)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Start ingesting data",
   "id": "623a551bf943bc4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T01:48:57.942388Z",
     "start_time": "2025-08-30T01:47:31.345843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Starting ingestion into PostgreSQL with connection: {jdbc_url}')\n",
    "\n",
    "write = (df.write.format(\"jdbc\")\n",
    "         .option(\"url\", jdbc_url)\n",
    "         .option(\"dbtable\", target_table)\n",
    "         .option(\"user\", db_user)\n",
    "         .option(\"password\", db_password)\n",
    "         .option(\"driver\", \"org.postgresql.Driver\")\n",
    "         #.option(\"batchsize\", 10000)\n",
    "         .option(\"truncate\", \"true\")\n",
    "         .option(\"createTableOptions\", \"WITH (OIDS=FALSE)\")\n",
    "         # Create table options (used if the table doesn't exist and Spark creates it):\n",
    "         )\n",
    "\n",
    "write.mode(\"append\").save()\n",
    "print(\"Done!\")"
   ],
   "id": "e7102bde0461dd83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ingestion into PostgreSQL with connection: jdbc:postgresql://222.255.214.74:5435/mydb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Shutdown session",
   "id": "86ed27551fbd324"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T01:48:58.285054Z",
     "start_time": "2025-08-30T01:48:58.027845Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "9fdaa077ff1c2f43",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
